% ============================================================================
%  Persistent Homology for Parameter-Free Spectral Line Decomposition
%  AASTeX v6.3.1 — ApJ pre-print
% ============================================================================
\documentclass[twocolumn,twocolappendix]{aastex631}

% --- Packages ---------------------------------------------------------------
\usepackage{amsmath,amssymb}
\usepackage{graphicx}
\usepackage{algorithmic}
\usepackage{booktabs}
\usepackage{xcolor}


% --- Macros -----------------------------------------------------------------
\newcommand{\phspectra}{\textsc{PHSpectra}}
\newcommand{\gausspyp}{\textsc{GaussPy+}}
\newcommand{\gausspy}{\textsc{GaussPy}}
\newcommand{\rohsa}{\textsc{ROHSA}}
\newcommand{\srms}{\sigma_{\mathrm{rms}}}
\newcommand{\pmin}{\pi_{\mathrm{min}}}
\newcommand{\aicc}{\mathrm{AICc}}

% --- Metadata ---------------------------------------------------------------
\shorttitle{Persistent Homology for Spectral Line Decomposition}
\shortauthors{Vera-Ciro}

% ============================================================================
\begin{document}

\title{Persistent Homology for Parameter-Free Spectral Line Decomposition}

\author{Carlos Vera-Ciro}
\affiliation{Independent Researcher}

% ---------------------------------------------------------------------------
% ABSTRACT
% ---------------------------------------------------------------------------
\begin{abstract}
Modern radio surveys routinely deliver millions of spectra, making automated
Gaussian decomposition a critical bottleneck in extracting physical insight
about the interstellar medium.  This work presents \phspectra, a parameter-light
approach that uses
0-dimensional persistent homology to detect peaks in 1D spectra.
Persistent homology tracks the birth and death of connected components in the
signal's upper-level sets as a threshold descends from the global maximum,
assigning each peak a topological persistence that measures its significance
without smoothing or differentiation.  The sole free parameter is $\beta$,
which sets the persistence threshold in units of noise:
$\pmin = \beta\,\srms$.  On a synthetic benchmark of 350 spectra spanning
seven difficulty categories, \phspectra\ achieves an overall $F_1$ score of
0.925 with performance varying by only 0.040
across $2.0 \leq \beta \leq 4.5$.  On 4200 real ${}^{13}$CO spectra from the
Galactic Ring Survey, the method delivers reliable fits at survey scale while
maintaining a single, interpretable significance threshold.  The result is a
survey-ready decomposition method that is robust to tuning choices and
scalable to modern datasets.
\end{abstract}

\keywords{methods: data analysis --- ISM: kinematics and dynamics ---
          techniques: spectroscopic --- radio lines: ISM}

% ============================================================================
% §1  INTRODUCTION
% ============================================================================
\section{Introduction}\label{sec:intro}

Radio spectral lines --- HI 21\,cm, ${}^{13}$CO, and other molecular
tracers --- encode the kinematic structure of the interstellar medium along
each line of sight.  A single spectrum is typically a superposition of
Gaussian components, each arising from a distinct gas cloud with its own
velocity, velocity dispersion, and column density.  Recovering these
components from a blended spectrum is a blind decomposition problem: one
must determine the number of components~$N_{\mathrm{components}}$ and the three parameters
(amplitude, mean, width) of each without prior knowledge of the true model.
Modern Galactic plane surveys such as the GRS \citep{Jackson2006},
GALFA-HI \citep{Peek2018}, THOR \citep{Beuther2016}, and SEDIGISM
\citep{Schuller2021} produce millions of spectra, making fully automated
decomposition essential.

A variety of automated approaches have been developed, each with distinct
trade-offs.  Derivative spectroscopy methods such as \gausspy\
\citep{Lindner2015} and \gausspyp\ \citep{Riener2019} identify peaks via
smoothed derivatives, but require tuning of smoothing scales $\alpha_1$ and
$\alpha_2$ to each survey, aiming to recover the scales of features in the observations.
Semi-interactive
toolkits such as \textsc{scousepy} \citep{Henshaw2016Scousepy} and
\textsc{pyspeckit} \citep{Ginsburg2011} enable human-in-the-loop fitting,
but scaling these workflows to millions of spectra is labor-intensive.
Probabilistic component detection methods such as \textsc{PySpecNest}
\citep{Sokolov2020} provide rigorous uncertainty quantification at the cost
of high per-spectrum computation.  GPU-accelerated fitters such as
\textsc{SPIF} \citep{Colombo2024} and survey-specific pipelines
(e.g., \textsc{mwydyn}; \citealt{Rigby2024}) target high throughput but rely
on specialized implementations.  Deep reinforcement learning
(\textsc{Spectuner-D1}; \citealt{Li2025}) learns a decomposition policy from
labeled training spectra and can be sensitive to domain shift when the
training set does not match survey conditions.
Regularized optimization (\rohsa; \citealt{Marchal2019}) enforces spatial
coherence in data cubes but introduces regularization weights that must be
tuned.  The common thread is that existing methods require multiple
parameters or training choices that depend on the data.

This paper presents \phspectra, a new approach that uses
0-dimensional persistent homology for peak detection in spectral-line
decomposition
\citep{Edelsbrunner2002,Zomorodian2005}.  Topological data analysis (TDA)
remains relatively niche in astronomy, with representative examples in
cosmic-web topology \citep{Xu2019} and cosmic-shear statistics
\citep{Heydenreich2022}; closely related structure-finding tools such as
\textsc{SUBFIND} \citep{Springel2001} have long been standard in simulations.
To the best of my knowledge, this is the first application
of persistent homology to spectral line decomposition.  The algorithm has a
single free parameter~$\beta$ (the persistence threshold in units of noise),
requires no expensive training, and runs in $O(n \log n)$ time.  It is shown that it
achieves high accuracy on synthetic data while matching or exceeding
\gausspyp\ on real GRS spectra without training.  The method is presented in
\S\ref{sec:method}, evaluated on synthetic and real ${}^{13}$CO spectra in
\S\ref{sec:evaluation}, discussed in the context of existing tools in
\S\ref{sec:discussion}, and summarized in \S\ref{sec:summary}.

% ============================================================================
% §2  METHOD
% ============================================================================
\section{Method}\label{sec:method}


\begin{figure*}[t!]
  \centering
  \includegraphics[width=0.9\textwidth]{water-level-stages.png}
  \caption{%
    Intuition for the peak-ranking step that underpins the decomposition.  The
    descending water-level interpretation of 0-dimensional persistent
    homology is applied to a synthetic three-peak spectrum.  As the threshold
    $t$ (dashed line) decreases, connected components are born at local maxima
    and die when they merge into an older component.  Red dots mark peak births;
    annotations show the final persistence $\pi = b - d$.  \emph{Top left}:
    $t$ above all peaks (empty upper-level set).  \emph{Top right}: peak~A
    ($\pi = 2.58$) is born as the global maximum.  \emph{Bottom left}: peak~B
    ($\pi = 1.20$) appears as a second island.  \emph{Bottom right}: peak~C
    ($\pi = 0.73$) dies when its island merges with~A's.
    \label{fig:waterlevel}}
\end{figure*}

The \phspectra\ algorithm is described in four stages: peak detection via
persistent homology (\S\ref{sec:persistence}), conversion of topological peaks
to Gaussian candidates (\S\ref{sec:candidates}), nonlinear least-squares
fitting (\S\ref{sec:fitting}), and iterative refinement guided by the
corrected Akaike Information Criterion (\S\ref{sec:refinement}).

% ---------------------------------------------------------------------------
% §2.1  Persistent homology for 1D peak detection
% ---------------------------------------------------------------------------
\subsection{Persistent homology for 1D peak detection}\label{sec:persistence}

Given a discrete 1D signal $f : \{0, 1, \ldots, n{-}1\} \to \mathbb{R}$, the method
defines the \emph{upper-level set} at threshold $t$ as
\begin{equation}
  U_t = \bigl\{\, i \;\big|\; f(i) \geq t \,\bigr\}.
\end{equation}
As $t$ decreases from $\max(f)$ toward $-\infty$, the topology of $U_t$
changes: new connected components are \emph{born} when $t$ drops below a
local maximum, and two components \emph{merge} when their supporting
intervals connect.  At each merger, the younger component --- the one whose
maximum is lower --- \emph{dies} at the current threshold.  This is
0-dimensional persistent homology ($H_0$): tracking connected components
across a filtration parameterized by $t$
\citep{Edelsbrunner2002,Edelsbrunner2008}.

Each local maximum (except the global one) produces a birth--death pair
$(b, d)$ where $b = f(i)$ is the peak height, $d$ is the merge threshold,
and the \emph{persistence}
\begin{equation}\label{eq:persistence}
  \pi = b - d
\end{equation}
quantifies how prominent the peak is.  The global maximum never merges; it
has $d = 0$ and persistence equal to its height.  Persistence provides a
scale-independent ranking of peaks: high-persistence features correspond to
real signal; low-persistence features are noise fluctuations.  No smoothing
or differentiation is required.
Implementation is straightforward: samples are processed in order of
decreasing function value, and a union-find data structure tracks connected
components.  When a newly visited index has an already-visited neighbor
belonging to a different component, the two components are merged and the
younger one's death is recorded.  Sorting the $n$ samples costs
$O(n \log n)$; each union-find operation is amortized $O(\alpha(n))$, where
$\alpha$ is the inverse Ackermann function, so the total complexity is
dominated by the initial sort.

Raw persistence is measured in the same units as the signal, a fixed
threshold cannot distinguish noise from signal across datasets with different
noise levels.  The minimum persistence is therefore set in units of the
noise:
\begin{equation}\label{eq:threshold}
  \pmin = \beta\,\srms,
\end{equation}
where $\srms$ is the noise RMS estimated from the data (see
\S\ref{sec:candidates}) and $\beta$ is the sole free parameter of the model.
The default value $\beta = 3.5$ corresponds to a $3.5\sigma$ persistence cut.\\

The name of this approach (Persistent Homology) reflects two ideas.  \emph{Homology} refers to the
algebraic-topological invariants that count features of different
dimensions: connected components ($H_0$), loops ($H_1$), voids ($H_2$),
and so on \citep{Zomorodian2005}.  For 1D peak detection only $H_0$ is
needed: each connected component of the upper-level set corresponds to a
peak.  \emph{Persistent} means that these invariants are tracked across the
full filtration parameterized by~$t$, not evaluated at a single threshold.
The lifetime of each component --- its persistence --- provides a
parameter-free ranking of feature significance that is stable under
perturbation \citep{Edelsbrunner2002,Otter2017}.  The union-find data
structure \citep{Tarjan1975} makes the computation efficient.

This suits spectral decomposition; in contrast with most methods that must commit early to
either a model family, a smoothing scale, or a heuristic for deciding which
peaks are worth fitting, all of which can bias the decomposition when lines
are blended or span a wide range of widths.  Persistent homology avoids a
single-scale commitment by descending through all thresholds, detecting peaks
at every scale simultaneously and ranking them by a single intrinsic measure
--- persistence --- that is directly comparable across features of different
amplitudes and widths.  A faint broad component and a bright narrow component
are treated on equal footing; no special tuning is required to accommodate
both.

Figure~\ref{fig:waterlevel} uses the descending water-level interpretation on
a synthetic three-peak spectrum.  As the water level (threshold) drops past
each local maximum, a new connected component (``island'') is born; when two
islands merge, the younger one dies and its persistence is recorded.  The
three real peaks have persistence values of $\pi = 2.58$, $1.20$, and $0.73$
--- well separated from the near-zero persistence of noise fluctuations.  In
practice these values are compared against the minimum persistence
$\pmin = \beta\,\srms$: peaks with $\pi > \pmin$ are kept as Gaussian
candidates, while lower-persistence features are rejected as noise.

% ---------------------------------------------------------------------------
% §2.2  From persistence to Gaussian candidates
% ---------------------------------------------------------------------------
\subsection{From persistence to Gaussian candidates}\label{sec:candidates}

Before peak detection, \phspectra\ estimates $\srms$ using a signal-masked
approach following \citet{Riener2019}, Section~3.1.1.  Runs of more than two
consecutive positive channels are masked (with two-channel padding on each
side) to exclude spectral features.  The median absolute deviation (MAD) of
the remaining negative channels provides an initial robust scale estimate.
Channels exceeding $\pm 5\,\sigma_{\mathrm{MAD}}$ are then clipped, and the
final RMS is computed as $\srms^2 = \sum_i f_i^2/N_{\mathrm{sur}}$, where the sum
runs over the $N_{\mathrm{sur}}$ surviving channels.  This procedure avoids biasing the noise estimate with
real emission and provides the noise scale used to set the persistence
threshold $\pmin = \beta\,\srms$.

Each peak surviving this threshold becomes an initial Gaussian guess.  The
amplitude is taken directly from the peak height, $a_0 = f(\mathrm{index})$,
and the mean is the peak channel index.  For non-global peaks, the initial
width is estimated from the persistence birth--death pair,
\begin{equation}\label{eq:sigma0}
  \sigma_0 = \frac{d}{\sqrt{2\ln(b / d_v)}},
\end{equation}
where $b$ is the peak birth value, $d_v$ the death value (the threshold at
which the peak merges with a higher component), and $d$ is the channel
distance between the peak and the merge point.  For the global maximum (which
has no death event), $\sigma_0 = 1.0$ channel is used as a fallback.  Peaks
are ordered by persistence (most significant first).  When the number of
candidates exceeds the internal parameter limit of the least-squares solver,
they are fitted in batches: the most persistent peaks are fitted against the
full signal first, then remaining peaks are fitted against the residual, and
all surviving components are combined into a single joint refit.

% ---------------------------------------------------------------------------
% §2.3  Gaussian fitting
% ---------------------------------------------------------------------------
\subsection{Gaussian fitting}\label{sec:fitting}

The initial guesses are fitted simultaneously as a sum of $K$ (the number of peaks
that survive the persistence threshold) Gaussians:
\begin{equation}\label{eq:model}
  F(x) = \sum_{i=1}^{K} a_i \exp\!\left(-\frac{(x - \mu_i)^2}{2\sigma_i^2}\right),
\end{equation}
using bounded nonlinear least-squares optimization.  The primary solver is a
custom C extension implementing the Levenberg--Marquardt algorithm with
analytic Jacobians, which provides the tight per-call overhead needed for
survey-scale processing.  If the C extension is unavailable (e.g.\ in a
pure-Python installation), \phspectra\ falls back to
\texttt{scipy.optimize.curve\_fit} \citep{Virtanen2020}.  In both cases the
bounds enforce $a_i \geq 0$, $\mu_i \in [0, n)$, and
$\sigma_i \in [0.3, n/2]$ channels.  Because the persistence-detected peak
positions and amplitudes are already close to the true values, the optimizer
typically converges in few iterations --- its primary task is to determine
the correct widths and fine-tune positions and amplitudes.

% ---------------------------------------------------------------------------
% §2.4  Iterative refinement
% ---------------------------------------------------------------------------
\subsection{Iterative refinement}\label{sec:refinement}

After the initial fit, components are validated once: a component is
rejected if any of the following hold: (a)~FWHM $< 1$ channel; (b)~mean
outside the spectrum; (c)~amplitude $< 1.5\,\srms$ (SNR floor); or
(d)~matched-filter signal-to-noise ratio $< 5.0$, where the matched-filter
SNR of component~$i$ is defined as
\begin{equation}\label{eq:snr_mf}
  \mathrm{SNR}_{\mathrm{mf},i} = \frac{a_i}{\srms}\,
    \sqrt{\sigma_i}\;\pi^{1/4}.
\end{equation}
This quantity measures the detectability of a Gaussian component of
amplitude~$a_i$ and width~$\sigma_i$ against noise of level~$\srms$.
If any components are rejected, the remaining ones are refit.

The validated model is then scored using the corrected Akaike Information
Criterion \citep{Akaike1974,Hurvich1989}:
\begin{equation}\label{eq:aicc}
  \aicc = N \ln\!\left(\frac{\mathrm{RSS}}{N}\right) + 2k
        + \frac{2k^2 + 2k}{N - k - 1},
\end{equation}
where $\mathrm{RSS} = \sum_j (f_j - F_j)^2$ is the residual sum of squares,
$k = 3K$ is the number of free parameters (three per Gaussian component),
and $N$ is the number of spectral channels.  If no residual peaks or blended
pairs are found, the result is returned without further refinement.

Otherwise, the model enters an iterative refinement loop of up to three
iterations.  Each iteration performs three operations:

\begin{enumerate}
  \item Residual peak search:  The persistence detection algorithm is
    ran on the residual spectrum (data minus model) with a lower threshold of
    $1.5\,\srms$ to find missed components.  Any peaks found are validated and
    added to the model.

  \item Negative dip splitting:  If the residual contains a dip
    exceeding $-5\,\srms$, the broadest Gaussian component overlapping that
    channel is split into two narrower components.

  \item Blended pair merging:  If two components are separated by
    less than $1.2 \times \min(\mathrm{FWHM}_i, \mathrm{FWHM}_j)$, they are
    merged into a single flux-weighted component.
\end{enumerate}

After each operation, the full model is refit via least-squares, and the
modification is accepted only if $\aicc$ decreases.  The loop terminates when
no operation produces an improvement or the iteration limit is reached.

Algorithm~\ref{alg:pipeline} summarizes the complete pipeline from raw spectrum
to final Gaussian components.

\begin{figure}[t!]
\begin{algorithmic}[1]
\REQUIRE Spectrum $f$, threshold parameter $\beta$
\ENSURE Set of Gaussian components $\mathcal{G}$
\STATE $\srms \leftarrow$ \textsc{EstimateNoise}($f$)
\STATE $\pmin \leftarrow \beta \cdot \srms$
\STATE peaks $\leftarrow$ \textsc{PersistencePeakDetect}($f$, $\pmin$)
\STATE $\mathcal{G} \leftarrow$ \textsc{InitialGuesses}(peaks)
\STATE $\mathcal{G} \leftarrow$ \textsc{LeastSquaresFit}($f$, $\mathcal{G}$)
\STATE $\mathcal{G} \leftarrow$ \textsc{ValidateComponents}($\mathcal{G}$, $\srms$)
\STATE $\aicc_{\mathrm{best}} \leftarrow$ \textsc{ComputeAICc}($f$, $\mathcal{G}$)
\IF{no residual peaks \AND no blended pairs}
  \RETURN $\mathcal{G}$
\ENDIF
\FOR{$i = 1$ \TO $3$}
  \STATE $r \leftarrow f - F(\mathcal{G})$
  \STATE $\mathcal{G}' \leftarrow \mathcal{G}$
  \STATE \textsc{SearchResidualPeaks}($r$, $1.5\,\srms$, $\mathcal{G}'$)
  \STATE \textsc{SplitAtNegativeDip}($r$, $5\,\srms$, $\mathcal{G}'$)
  \STATE \textsc{MergeBlendedPairs}($\mathcal{G}'$)
  \STATE $\mathcal{G}' \leftarrow$ \textsc{LeastSquaresFit}($f$, $\mathcal{G}'$)
  \IF{\textsc{ComputeAICc}($f$, $\mathcal{G}'$) $< \aicc_{\mathrm{best}}$}
    \STATE $\mathcal{G} \leftarrow \mathcal{G}'$
    \STATE $\aicc_{\mathrm{best}} \leftarrow$ \textsc{ComputeAICc}($f$, $\mathcal{G}'$)
  \ELSE
    \RETURN $\mathcal{G}$
  \ENDIF
\ENDFOR
\RETURN $\mathcal{G}$
\end{algorithmic}
\caption{\phspectra\ pipeline.\label{alg:pipeline}}
\end{figure}


% ============================================================================
% §3  EVALUATION
% ============================================================================
\section{Evaluation}\label{sec:evaluation}

\begin{figure}[t!]
  \centering
  \includegraphics[width=\columnwidth]{synthetic-f1.png}
  \caption{%
    Robustness to the sole free parameter.  $F_1$ score versus $\beta$ for each
    synthetic benchmark category (colored lines) and overall (black).
    Performance is stable: the overall $F_1$ varies by only 0.040 across
    $\beta \in [2.0, 4.5]$.
    \label{fig:synthetic_f1}}
\end{figure}


\phspectra\ is evaluated on both synthetic spectra with known ground-truth
components (\S\ref{sec:synthetic}) and real ${}^{13}$CO data from the Galactic
Ring Survey (\S\ref{sec:grs}).

\begin{deluxetable*}{llccclc}
\tablecaption{Synthetic benchmark categories.  Each category contains 50
spectra with the listed parameter ranges and constraints.  All spectra have
424 channels and noise $\sigma = 0.25$~K.  The last column gives the $F_1$
score at the optimal $\beta$.\label{tab:categories}}
\tablehead{
  \colhead{Category} & \colhead{Label} & \colhead{$N_{\mathrm{comp}}$} &
  \colhead{$a$ (K)} & \colhead{$\sigma$ (ch)} & \colhead{Constraint} &
  \colhead{$F_1$}
}
\startdata
Single Bright    & SB  & 1   & 1.0--5.0  & 3--10  & SNR $> 7$                & 0.962 \\
Single Faint     & SF  & 1   & 0.3--0.8  & 3--10  & SNR $2$--$6$             & 0.907 \\
Single Narrow    & SN  & 1   & 1.0--5.0  & 1--2.5 & Sub-resolution           & 0.943 \\
Single Broad     & SBd & 1   & 0.5--3.0  & 10--20 & Extended features        & 0.952 \\
Multi Separated  & MS  & 2--3 & 0.5--4.0 & 2--8   & Sep.\ $> 4\sigma$       & 0.975 \\
Multi Blended    & MB  & 2--3 & 0.5--4.0 & 3--8   & Sep.\ $1.5$--$3\sigma$  & 0.820 \\
Crowded          & C   & 4--5 & 0.3--3.0 & 2--6   & Mixed separations        & 0.936 \\
\enddata
\end{deluxetable*}

% ---------------------------------------------------------------------------
% §3.1  Synthetic benchmark
% ---------------------------------------------------------------------------
\subsection{Synthetic benchmark}\label{sec:synthetic}

A benchmark of 350 synthetic spectra ($50$ per category) was constructed to
span seven categories of increasing difficulty (Table~\ref{tab:categories}).
Each spectrum has 424 channels with additive Gaussian noise at
$\sigma = 0.25$~K, matching the properties of the GRS \citep{Jackson2006}.
Because the true Gaussian components are known exactly, the $F_1$ score measures
true accuracy rather than agreement with another algorithm.  Component
matching uses the Hungarian algorithm with criteria adapted from
\citet{Lindner2015}: a predicted component matches a true component if the
amplitude ratio is in $(0, 10)$, the position difference is less than
$1\sigma$ of the true component, and the width ratio is in $(0.3, 2.5)$.
$F_1$ is computed as the harmonic mean of precision (fraction of predicted
components that match a true one) and recall (fraction of true components that
are matched).

Figure~\ref{fig:synthetic_f1} shows the $F_1$ score as a function of $\beta$ for
each category and overall measurement.  The overall $F_1$ varies by only 0.040 across the
full sweep from $\beta = 2.0$ to $\beta = 4.5$, confirming the low
sensitivity of the algorithm to this parameter.  At the optimal
$\beta = 2.8$, the overall $F_1$ is 0.925.  The difficulty gradient follows
physical expectations: well-separated multi-component spectra (MS) are
easiest ($F_1$~$= 0.975$), while heavily blended multi-component spectra (MB) are
the most challenging ($F_1$~$= 0.820$), a regime where any algorithm faces
fundamental ambiguity due to overlapping components.  Per-category $F_1$
scores are listed in the last column of Table~\ref{tab:categories}.


Across the $\beta$ sweep, position errors are typically sub-channel, and
amplitude and width relative errors remain small and stable.  These trends
hold across all seven categories.\\

Against real training data the behavior is similar, $F_1$ is highly insensitive
to $\beta$ accross a wide range, with an optimal value at around $\approx 3$.
The practical reason for preferring $\beta=3.5$ is speed. A lower $\beta$ admits
more candidate peaks from the persistence filtration, many of which sit just
above the noise floor. These marginal peaks generate initial Gaussian guesses
that must be fitted by the Levenberg-Marquardt solver -- the most expensive 
step in the pipeline -- only to be discarded during component validation 
(SNR floor, matched-filter SNR). The fitting cost scales with the number of
components, so a large number of doomed candidates slows the algorithm without
improving the final decomposition.\\

At $\beta = 3.5$, the persistence threshold is high enough that most noise 
peaks are rejected before fitting, while genuine features are retained. 
This provides a good balance between not missing real peaks near the detection 
limit and not wasting computation on candidates that will be removed downstream.

% ---------------------------------------------------------------------------
% §3.2  GRS comparison with GaussPy+
% ---------------------------------------------------------------------------
\subsection{GRS comparison with GaussPy+}\label{sec:grs}

\phspectra\ and \gausspyp\ are compared on all 4200 spectra in a GRS test field
from the Galactic Ring Survey \citep[GRS;][]{Jackson2006}, a ${}^{13}$CO
$J = 1 \to 0$ survey of the inner Galaxy.  Each spectrum has 424 velocity
channels.  \gausspyp\ runs with the trained parameters from
\citet{Riener2019}: $\alpha_1 = 2.89$, $\alpha_2 = 6.65$ (two-phase
decomposition), and an SNR threshold of 3.0.  \phspectra\ uses the default
$\beta = 3.5$.


Both tools achieve nearly identical mean residual RMS (0.136~K for
\phspectra\ vs.\ 0.135~K for \gausspyp).  In head-to-head comparisons,
\phspectra\ achieves lower residual RMS on 62\% of spectra (2592 of 4200).
Figure~\ref{fig:rms_scatter} shows a scatter plot of per-spectrum RMS
values; the majority of points fall below the 1:1 line, confirming lower
\phspectra\ residuals.  The slightly lower mean RMS of \gausspyp\ is driven
by a tail of spectra where it fits many more components, reducing RMS at the
cost of potential overfitting.  Splitting the 4200 spectra at a residual
RMS threshold of 0.2~K reveals two distinct regimes.  For low-noise spectra
(RMS~$\leq 0.2$~K, 3814 spectra) the mean component counts are comparable:
$\langle N \rangle = 2.6$ for \phspectra\ vs.\ 1.9 for \gausspyp.  For
noisy spectra (RMS~$> 0.2$~K, 386 spectra), the two tools diverge sharply:
\gausspyp\ fits $\langle N \rangle = 8.2$ components on average compared to
1.6 for \phspectra.  The additional free parameters mechanically reduce the
residual --- more Gaussians always lower $\chi^2$ --- but fitting $8+$
components to a noisy spectrum is more likely to trace noise structure than
real emission.  \phspectra's persistence threshold imposes a hard
significance floor: no candidate peak survives unless its topological
prominence exceeds $\beta\,\srms$, so on noisy spectra fewer (or
zero) components are fitted, producing higher RMS but a more physically
defensible decomposition.  This interpretation is specific to the
\gausspyp\ comparison here; other decomposition methods may trade residual
fit quality against component count in different ways.

\begin{figure}[t!]
  \centering
  \includegraphics[width=\columnwidth]{rms-scatter.png}
  \caption{%
    Head-to-head comparison on real data.  Per-spectrum residual RMS:
    \phspectra\ versus \gausspyp.  Points below the 1:1 line (dashed) indicate
    spectra where \phspectra\ achieves lower residual RMS.
    \label{fig:rms_scatter}}
\end{figure}

Across 6838 matched component pairs (Hungarian matching with a $2\sigma$
position tolerance), the median log-width ratio
$\ln(\sigma_{\mathrm{PHSpectra}} / \sigma_{\mathrm{GaussPy+}})$ is near zero, indicating
no systematic width bias between the two tools.


% ---------------------------------------------------------------------------
% §3.3  Survey-scale application
% ---------------------------------------------------------------------------
\subsection{Survey-scale application}\label{sec:survey}

To demonstrate \phspectra\ at survey scale, five GRS tiles
(tiles 26, 28, 30, 32, 34; $\ell \approx 25\degr$--$35\degr$) are
decomposed.  These sightlines cross the Scutum--Centaurus arm tangent
and several intervening features, where molecular cloud complexes are
densely packed along the line of sight.  Each tile is processed through the
serverless pipeline described in \S\ref{sec:discussion}, yielding a
Gaussian decomposition for every spatial pixel.

Figure~\ref{fig:grs_map} shows four scalar fields derived from the
decomposition.  Panel~(a) maps three velocity bins to RGB channels,
revealing the kinematic layering of distinct gas populations along the line
of sight.  Panel~(b) shows the topological complexity --- the number of
Gaussian components per pixel --- which traces cloud boundaries, outflow
regions, and shock fronts.  Panel~(c) encodes centroid velocity as hue and
peak amplitude as luminance, combining kinematic and intensity information
in a single view.  Panel~(d) displays the centroid velocity of the brightest
component per pixel, recovering bulk gas motions that are blurred by
moment-1 maps when multiple clouds overlap.

\begin{figure*}[t!]
  \centering
  \includegraphics[width=\textwidth]{grs-map-plot.png}
  \caption{%
    Survey-scale products derived from the decomposition (GRS tiles 26--34;
    ($\ell \approx 25\degr$--$35\degr$).  \emph{(a)}~Velocity RGB
    composite: three velocity bins mapped to R, G, B from the decomposed
    Gaussians.  \emph{(b)}~Topological complexity: number of Gaussian
    components per pixel.  \emph{(c)}~Amplitude--velocity bivariate
    colormap: hue encodes centroid velocity, luminance encodes peak
    amplitude.  \emph{(d)}~Dominant velocity field: centroid velocity of
    the brightest component per pixel.
    \label{fig:grs_map}}
\end{figure*}

Velocity-spacing statistics provide a complementary view of line-of-sight
structure (Figure~\ref{fig:velocity_spacing}).  For each spectrum, adjacent
components are ordered by centroid velocity and the separations $\Delta v$
are measured; the resulting distribution highlights common multi-layer
spacings and can be compared across regions with different crowding.  A
well-behaved, physically interpretable $\Delta v$ distribution indicates
that the decomposition is extracting coherent kinematic layers rather than
fitting noise.

\begin{figure}[t!]
  \centering
  \includegraphics[width=\columnwidth]{velocity-spacing-plot.png}
  \caption{%
    Normalized distributions
    of velocity separations $\Delta v$ between adjacent fitted components,
    shown for all spectra (solid) and split by component count per spectrum:
    $N = 2$, $N = 3$--4, and $N \ge 5$.  Small $\Delta v$
    dominates, while higher-component spectra show broader tails, indicating
    richer line-of-sight layering.}
  \label{fig:velocity_spacing}
\end{figure}

% ---------------------------------------------------------------------------
% §3.4  Computational performance
% ---------------------------------------------------------------------------
\subsection{Computational performance}\label{sec:performance}

A key practical advantage of \phspectra\ is its computational efficiency.
The wall-clock time for decomposing all 4200 GRS test-field spectra
(424 channels each) is measured on a single core with no parallelization
for either tool.  \phspectra\ uses the default $\beta = 3.5$ with its
C-accelerated Levenberg--Marquardt solver; \gausspyp\ is run in Docker with
Python~3.10 (required for compatibility with its dependencies).

Table~\ref{tab:timing} summarizes the results.  The typical per-spectrum
execution time for \phspectra\ is remarkably low: the median is just
51.6\,ms, placing the algorithm firmly in the regime where survey-scale
processing of millions of spectra becomes routine on modest hardware.
In total wall-clock time \phspectra\ is $2.0\times$ faster (746.4\,s vs.\
1477.4\,s).  While the two tools have comparable median latencies
(51.6\,ms vs.\ 40.4\,ms), their timing distributions diverge sharply in
the tails: at the 95th percentile both are still similar (705\,ms vs.\
703\,ms), but \phspectra's P99 is 3.0\,s compared to \gausspyp's 8.4\,s
--- a $2.8\times$ difference.  These extreme outliers dominate the
aggregate timing and are the primary reason for the $2\times$ total
speedup despite comparable typical performance.

Three algorithmic properties underpin this efficiency.  First, \phspectra\
operates directly on the raw spectrum using persistence-based peak
detection, bypassing the smoothing sweep that \gausspyp\ performs at each
$\alpha$ scale.  Second, the custom C extension for both the
Levenberg--Marquardt solver and the union-find persistence computation
keeps per-call overhead to the 40--60\,ms range for typical spectra.
Third, \phspectra\ requires no separate training step --- \gausspyp's
$\alpha$ parameters must be trained per survey on synthetic data, adding
computational cost not reflected in the per-spectrum timing.

\begin{deluxetable}{lccc}
\tablecaption{Wall-clock timing benchmark on 4200 GRS spectra.
\label{tab:timing}}
\tablehead{
  \colhead{Metric} & \colhead{\phspectra} & \colhead{\gausspyp} &
  \colhead{Factor}
}
\startdata
Total time          & 746.4\,s   & 1477.4\,s  & $2.0\times$ \\
Mean per spectrum   & 177.7\,ms  & 350.5\,ms  & $2.0\times$ \\
Median per spectrum & 51.6\,ms   & 40.4\,ms   & ---         \\
P95 per spectrum    & 705.4\,ms  & 702.5\,ms  & ---         \\
P99 per spectrum    & 3.0\,s     & 8.4\,s     & $2.8\times$ \\
Components found    & 2.48       & 2.44       & ---         \\
\enddata
\end{deluxetable}



% ============================================================================
% §4  DISCUSSION
% ============================================================================
\section{Discussion}\label{sec:discussion}

Table~\ref{tab:comparison} places \phspectra\ in the context of three
established decomposition tools.  The distinguishing features are the use
of persistent homology for peak detection, the absence of training
requirements, and the intrinsic multi-scale nature of the persistence
filtration.  \gausspyp\ offers spatial coherence that \phspectra\ currently
lacks, while \rohsa\ provides full regularized optimization with built-in
coherence but requires manual tuning of regularization weights.

The physical value of the decomposition is clearest at survey scale.  The
component-count map provides a compact proxy for structural complexity,
flagging regions where overlapping clouds, shocks, or feedback increase the
number of kinematic layers.  Spatial coherence in the fitted components
preserves cloud- and filament-scale morphology, allowing kinematic structures
to be traced across the cube without manual tuning.  Velocity-spacing
statistics add a one-dimensional summary of line-of-sight layering, revealing
characteristic separations between components that can be compared across
environments.

\paragraph{Scalability.}
Because each spectrum is decomposed independently, the algorithm is
trivially parallelizable.  A serverless pipeline has been deployed on
AWS Lambda that processes the full GRS ($\sim$2.3 million spectra) for
$\sim$\$40 in compute cost; the details of this infrastructure are beyond
the scope of this paper.

\paragraph{Limitations.}
Two limitations of the current approach should be noted; they are framed as
directions for future work.  First, \phspectra\ treats each spectrum
independently and does not enforce spatial coherence across neighboring
pixels.  \gausspyp\ uses a spatial refitting stage \citep[Section~3.3
of][]{Riener2019} that improves consistency across a data cube; a similar
post-processing step could be added to \phspectra.  Second, the algorithm assumes
Gaussian line profiles.  Non-Gaussian features such as self-absorption dips,
outflows, or Lorentzian wings are not modeled; extending the fitting stage
to support additional profile shapes is a natural next step.

\begin{deluxetable*}{lllll}
\tablecaption{Comparison of spectral line decomposition methods.
\label{tab:comparison}}
\tablehead{
  \colhead{Property} & \colhead{\phspectra} & \colhead{\gausspy} &
  \colhead{\gausspyp} & \colhead{\rohsa}
}
\startdata
Peak detection    & Persistent homology   & Derivative spectroscopy & Derivative spectroscopy & Regularized optimization \\
Free parameters   & $\beta$ only          & $\alpha_1$, $\alpha_2$  & $\alpha_1$, $\alpha_2$, SNR thresholds & $\lambda$ weights \\
Training          & None (default works)  & Required (synthetic)    & Required (synthetic)    & Manual tuning \\
Spatial coherence & No                    & No                      & Yes                     & Yes \\
Model selection   & AICc                  & BIC                     & BIC + heuristics        & Regularization \\
Multi-scale       & Intrinsic             & Per-$\alpha$ scale      & Per-$\alpha$ scale      & Regularized \\
Speed (GRS)       & 178\,ms/spectrum      & ---                     & 351\,ms/spectrum        & --- \\
\enddata
\end{deluxetable*}


% ============================================================================
% §5  SUMMARY
% ============================================================================
\section{Summary}\label{sec:summary}

\phspectra\ is presented as a new tool for automated Gaussian
decomposition of spectral lines based on 0-dimensional persistent homology.
The method detects peaks by tracking the birth and death of connected
components in the signal's upper-level sets as a threshold descends from the
global maximum, ranking features by their topological persistence without
smoothing or differentiation.  This yields a single-parameter model: the sole
free parameter is $\beta = 3.5$ (the persistence threshold in units of noise),
which requires no survey-specific training and is physically interpretable as
a significance threshold.

On a benchmark of 350 synthetic spectra spanning seven difficulty categories,
\phspectra\ achieves an overall $F_1$~$= 0.925$ at the optimal $\beta = 2.8$,
with performance varying by only 0.040 across $\beta \in [2.0, 4.5]$.  On
real data, the comparison with \gausspyp\ on 4200 GRS spectra shows comparable
mean residual RMS and lower RMS in 62\% of head-to-head cases; the spectra
where \gausspyp\ performs better are those where it fits many more components,
consistent with overfitting.  The pipeline is also faster, achieving a
$2.0\times$ reduction in total wall-clock time and a P99 latency of 3.0\,s
versus 8.4\,s for \gausspyp.

The software, documentation, and benchmark scripts are publicly available at
\url{https://github.com/caverac/phspectra}.

At survey scale, \phspectra\ turns spectra into physically interpretable
products: component-count maps that track complexity, spatially coherent
component fields that preserve morphology, and velocity-spacing statistics
that summarize line-of-sight layering.

The central result is that persistent homology enables a survey-ready,
parameter-light decomposition that matches trained methods while eliminating
their tuning burden, making large-scale spectral analysis faster, more
reproducible, and easier to deploy.

% ============================================================================
\bibliography{ms}
\bibliographystyle{aasjournal}

\end{document}
