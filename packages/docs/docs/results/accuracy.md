---
sidebar_position: 2
---

# Accuracy

Plots in this section can be reproduced using, both execute in under a couple of seconds

```bash
uv run benchmarks compare-plot
uv run benchmarks ncomp-rms-plot
```

## True accuracy on synthetic data

When ground truth is known exactly (synthetic spectra with prescribed Gaussian components), PHSpectra achieves an overall **$F_1$ = 0.899**. The only challenging regime is heavily blended multi-component spectra ($F_1$ = 0.749), where any algorithm faces fundamental ambiguity. See the [Beta parameter sensitivity](beta) section for the full breakdown.

## Comparison with GaussPy+

We run both PHSpectra and GaussPy+ on all 4200 spectra in the GRS test field. GaussPy+ is run in Docker using `GaussPyDecompose` with the trained parameters from [Riener et al. (2019)](https://arxiv.org/abs/1906.10506): $\alpha_1 = 2.89$, $\alpha_2 = 6.65$, two-phase decomposition, SNR threshold = 3.0.

### Fit quality (RMS)

| Metric         | PHSpectra             | GaussPy+          |
| -------------- | --------------------- | ----------------- |
| Mean RMS (K)   | 0.1356                | 0.1345            |
| Lower RMS wins | **2592 / 4200** (62%) | 1488 / 4200 (35%) |

The two tools achieve nearly identical mean RMS. PHSpectra achieves lower residuals on 62% of spectra in head-to-head comparisons.

<figure class="scientific">
  <img src="/img/results/rms-distribution.png" alt="RMS distribution" />
  <figcaption>

**Figure 1.** Residual RMS distributions for PHSpectra (solid) and GaussPy+ (dashed) on all 4200 GRS test-field spectra. Both distributions are bimodal, reflecting two distinct noise regimes in the survey field ($\sigma \approx 0.09$ K and $\sigma \approx 0.15$ K). Generated by `uv run benchmarks compare-plot`.

  </figcaption>
</figure>

The RMS distributions overlap heavily and both exhibit a bimodal structure. This bimodality is a property of the input data, not the fitting: the GRS test field contains two spatial populations with different noise levels ($\sigma \approx 0.09$ K and $\sigma \approx 0.15$ K), likely due to varying integration time or field-edge effects. Both tools' residual RMS tracks the local noise floor.

<figure class="scientific">
  <img src="/img/results/rms-scatter.png" alt="RMS scatter" />
  <figcaption>

**Figure 2.** Per-spectrum RMS scatter: each point is one of the 4200 spectra. Points below the 1:1 line (dashed) have lower PHSpectra RMS; points above have lower GaussPy+ RMS. The cloud is roughly symmetric around the diagonal, consistent with the near-even win split. Generated by `uv run benchmarks compare-plot`.

  </figcaption>
</figure>

The scatter plot (Figure 2) confirms the near-even split: the cloud of points is roughly symmetric around the 1:1 line, with neither tool dominating across the full RMS range.

### Where decompositions differ

A systematic comparison reveals several recurring patterns of disagreement:

<figure class="scientific">
  <img src="/img/results/compare-disagreements.png" alt="Disagreement cases" />
  <figcaption>

**Figure 3.** Six representative spectra where PHSpectra and GaussPy+ disagree, selected to cover different disagreement types. Each panel shows the raw spectrum (grey), PHSpectra fit (black, solid), and GaussPy+ fit (black, dashed) with individual components. Generated by `uv run benchmarks compare-plot`.

  </figcaption>
</figure>

The six panels in Figure 3 show representative cases:

- **PHS fewer components**: GaussPy+ sometimes fits many components (up to 14) where PHSpectra finds fewer, better-constrained ones
- **PHS more components**: PHSpectra resolves blended features that GaussPy+ misses entirely
- **PHS lower / GP+ lower RMS**: each tool wins on different spectra, with different decomposition strategies
- **Same N, different positions**: even with the same component count, the two algorithms place components differently
- **Different widths**: the two algorithms sometimes assign different widths to the same feature

### Component count vs RMS

The scatter plot below shows the number of fitted components against residual RMS for both methods.

<figure class="scientific">
  <img src="/img/results/ncomp-vs-rms.png" alt="N components vs RMS" />
  <figcaption>

**Figure 4.** Number of fitted components vs residual RMS for PHSpectra and GaussPy+ on all 4200 GRS test-field spectra. Generated by `uv run benchmarks ncomp-rms-plot`.

  </figcaption>
</figure>

Both tools detect comparable numbers of components on average (PHSpectra 2.35, GaussPy+ 2.44). PHSpectra's persistence threshold imposes a hard significance floor: no candidate peak survives unless its topological prominence exceeds $\beta \times \sigma_\mathrm{rms}$. On noisy spectra this means fewer (or zero) components are fitted, producing a higher RMS but a more physically defensible decomposition.

### Component widths

A population-level comparison of fitted widths shows **no systematic bias** between the two tools. Matching 6838 component pairs across 4200 spectra (Hungarian algorithm, position tolerance $< 2\sigma$), the median log-width ratio $\ln(\sigma_{\text{PHSpectra}} / \sigma_{\text{GaussPy+}})$ is near zero.

<figure class="scientific">
  <img src="/img/results/width-comparison.png" alt="Width comparison" />
  <figcaption>

**Figure 5.** Distribution of $\ln(\sigma_{\rm PHSpectra} / \sigma_{\rm GaussPy+})$ for 6838 matched component pairs across 4200 spectra. The distribution is sharply peaked at zero, confirming no systematic width bias between the two tools. Generated by `uv run benchmarks compare-plot`.

  </figcaption>
</figure>

The histogram in Figure 5 is sharply peaked at zero, confirming that neither tool systematically favours wider or narrower profiles. While individual spectra can show large width differences (the disagreement panel in Figure 3 includes such cases), these are isolated instances driven by different decomposition strategies, not a systematic effect.
