---
sidebar_position: 2
---

# Accuracy

Plots in this section can be reproduced using, both execute in under a couple of seconds

```bash
uv run benchmarks compare-plot
uv run benchmarks ncomp-rms-plot
```

## True accuracy on synthetic data

When ground truth is known exactly (synthetic spectra with prescribed Gaussian components), PHSpectra achieves an overall **$F_1$ = 0.899**. The only challenging regime is heavily blended multi-component spectra ($F_1$ = 0.749), where any algorithm faces fundamental ambiguity. See the [Beta parameter sensitivity](beta) section for the full breakdown.

## Comparison with GaussPy+

We run both PHSpectra and GaussPy+ on all 4200 spectra in the GRS test field. GaussPy+ is run in Docker using `GaussPyDecompose` with the trained parameters from [Riener et al. (2019)](https://arxiv.org/abs/1906.10506): $\alpha_1 = 2.89$, $\alpha_2 = 6.65$, two-phase decomposition, SNR threshold = 3.0.

### Fit quality (RMS)

| Metric         | PHSpectra         | GaussPy+              |
| -------------- | ----------------- | --------------------- |
| Mean RMS (K)   | 0.2229            | 0.2245                |
| Lower RMS wins | 1911 / 4200 (46%) | **2289 / 4200** (54%) |

The two tools achieve nearly identical mean RMS, with GaussPy+ winning slightly more head-to-head comparisons (54% vs 46%). The split is close to even, indicating that neither method has a systematic residual advantage over the other on real data.

<figure class="scientific">
  <img src="/img/results/rms-distribution.png" alt="RMS distribution" />
  <figcaption>

**Figure 1.** Residual RMS distributions for PHSpectra (solid) and GaussPy+ (dashed) on all 4200 GRS test-field spectra. Both distributions peak near the noise floor ($\sigma \approx 0.13$ K) and overlap heavily. Generated by `uv run benchmarks compare-plot`.

  </figcaption>
</figure>

The RMS distributions overlap heavily &mdash; both tools fit most spectra near the noise floor ($\sigma = 0.13$ K).

<figure class="scientific">
  <img src="/img/results/rms-scatter.png" alt="RMS scatter" />
  <figcaption>

**Figure 2.** Per-spectrum RMS scatter: each point is one of the 4200 spectra. Points below the 1:1 line (dashed) have lower PHSpectra RMS; points above have lower GaussPy+ RMS. The cloud is roughly symmetric around the diagonal, consistent with the near-even win split. Generated by `uv run benchmarks compare-plot`.

  </figcaption>
</figure>

The scatter plot (Figure 2) confirms the near-even split: the cloud of points is roughly symmetric around the 1:1 line, with neither tool dominating across the full RMS range.

### Where decompositions differ

A systematic comparison reveals several recurring patterns of disagreement:

<figure class="scientific">
  <img src="/img/results/compare-disagreements.png" alt="Disagreement cases" />
  <figcaption>

**Figure 3.** Six representative spectra where PHSpectra and GaussPy+ disagree, selected to cover different disagreement types. Each panel shows the raw spectrum (grey), PHSpectra fit (black, solid), and GaussPy+ fit (black, dashed) with individual components. Generated by `uv run benchmarks compare-plot`.

  </figcaption>
</figure>

The six panels in Figure 3 show representative cases:

- **PHS fewer components**: GaussPy+ sometimes fits many components (up to 14) where PHSpectra finds fewer, better-constrained ones
- **PHS more components**: PHSpectra resolves blended features that GaussPy+ misses entirely
- **PHS lower / GP+ lower RMS**: each tool wins on different spectra, with different decomposition strategies
- **Same N, different positions**: even with the same component count, the two algorithms place components differently
- **Different widths**: the two algorithms sometimes assign different widths to the same feature

### Component count vs RMS

The scatter plot below shows the number of fitted components against residual RMS for both methods. A clear pattern emerges above RMS $\approx 0.2$ K: GaussPy+ fits systematically more components to noisy spectra than PHSpectra does.

<figure class="scientific">
  <img src="/img/results/ncomp-vs-rms.png" alt="N components vs RMS" />
  <figcaption>

**Figure 4.** Number of fitted components vs residual RMS for PHSpectra and GaussPy+ on all 4200 GRS test-field spectra. Above RMS $\approx 0.2$ K, GaussPy+ fits systematically more components. Generated by `uv run benchmarks ncomp-rms-plot`.

  </figcaption>
</figure>

This is a potential overfitting problem (Figure 4). When a spectrum is noisy or has weak, ambiguous features, GaussPy+'s derivative-based detection can interpret noise fluctuations as real peaks, fitting many narrow components to chase down the residual. The result is a lower RMS &mdash; but at the cost of introducing spurious components that have no physical basis. This explains why GaussPy+ has a slightly lower _mean_ RMS: its mean is pulled down by a tail of high-noise spectra where it fits 10&ndash;15 components to what PHSpectra correctly identifies as noise.

PHSpectra's persistence threshold imposes a hard significance floor: no candidate peak survives unless its topological prominence exceeds $\beta \times \sigma_\mathrm{rms}$. On noisy spectra this means fewer (or zero) components are fitted, producing a higher RMS but a more physically defensible decomposition. In the low-RMS regime (RMS $\leq 0.2$ K), where both methods agree that real signal is present, the mean component counts are comparable.

### Component widths

A population-level comparison of fitted widths shows **no systematic bias** between the two tools. Matching 6697 component pairs across 4200 spectra (Hungarian algorithm, position tolerance $< 2\sigma$), the median log-width ratio $\ln(\sigma_{\text{PHSpectra}} / \sigma_{\text{GaussPy+}})$ is near zero, and the split is near even: GaussPy+ fits wider profiles in 55% of pairs, PHSpectra in 45%.

<figure class="scientific">
  <img src="/img/results/width-comparison.png" alt="Width comparison" />
  <figcaption>

**Figure 5.** Distribution of $\ln(\sigma_{\rm PHSpectra} / \sigma_{\rm GaussPy+})$ for 6697 matched component pairs across 4200 spectra. The distribution is sharply peaked at zero, confirming no systematic width bias between the two tools. Generated by `uv run benchmarks compare-plot`.

  </figcaption>
</figure>

The histogram in Figure 5 is sharply peaked at zero, confirming that neither tool systematically favours wider or narrower profiles. While individual spectra can show large width differences (the disagreement panel in Figure 3 includes such cases), these are isolated instances driven by different decomposition strategies, not a systematic effect.
