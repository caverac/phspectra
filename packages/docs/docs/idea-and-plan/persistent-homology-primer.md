---
sidebar_position: 2
---

# Persistent Homology

A detailed look at 0-dimensional persistent homology for 1D signals and how phspectra uses it to decompose spectra into Gaussian components.

## Mathematical foundation

The theory of persistent homology was introduced by [Edelsbrunner et al. (2002)](https://doi.org/10.1007/s00454-002-2885-2) and developed into a complete computational framework by [Zomorodian & Carlsson (2005)](https://doi.org/10.1007/s00454-004-1146-y). For a textbook treatment, see [Edelsbrunner & Harer (2010)](https://doi.org/10.1090/mbk/069). Here we restrict ourselves to the 0-dimensional case on 1D signals, which is all that phspectra needs.

### Upper-level sets and connected components

Given a 1D signal $f : \{0, 1, \ldots, n-1\} \to \mathbb{R}$, the **upper-level set** at threshold $t$ is

$$
U_t = \{ i \mid f(i) \geq t \}
$$

As $t$ decreases from $\max(f)$ toward $-\infty$, the topology of $U_t$ changes:

- **Birth**: when $t$ drops below a local maximum $f(i)$, a new connected component appears. We say the component is **born** at value $f(i)$.
- **Death**: when $t$ drops enough that two components merge, the **younger** component (the one whose maximum is lower) **dies** at that threshold. The elder component absorbs it.

This is **0-dimensional persistent homology** ($H_0$): we track connected components (dimension 0) across a filtration parameterized by $t$.

### Birth-death pairs and persistence

Each local maximum except the global one produces a pair $(b, d)$ where:

- $b = f(i)$ is the **birth** value (peak height)
- $d$ is the **death** value (threshold at which it merges into an older component)
- **Persistence** $\pi = b - d$ quantifies how prominent the peak is

The global maximum never merges into anything -- it has $d = 0$ and persistence equal to its height.

**Key property**: persistence provides a scale-independent ranking of peaks. High-persistence peaks correspond to real features; low-persistence peaks are noise fluctuations. No smoothing or derivative is needed.

### Complexity

For a 1D signal of length $n$, the algorithm sorts indices by decreasing value ($O(n \log n)$) and processes each with a union-find structure ([Tarjan 1975](https://doi.org/10.1145/321879.321884); amortized $O(\alpha(n))$ per operation). Total complexity is $O(n \log n)$.

## Visual intuition

### The descending water level

Think of the signal as a mountain range seen from the side, with water starting above all peaks. As the water drains:

The figure below shows this process on a synthetic three-peak spectrum (the same kind of signal phspectra is designed to decompose). The three peaks are labelled by decreasing amplitude:

| Peak | Channel | Amplitude | Role |
|------|---------|-----------|------|
| **A** | 60 | $\approx 2.5$ | Global maximum (tallest) |
| **B** | 100 | $\approx 1.4$ | Second tallest |
| **C** | 140 | $\approx 0.7$ | Weakest |

The blue shaded region represents the water, and the dashed line marks the current threshold $t$. Red dots appear at the moment a peak is born, annotated with its final persistence $\pi$.

![Water level stages](/img/results/water-level-stages.png)

*Generated by `uv run benchmarks persistence-plot`. See [Reproducing results](../results/reproducing).*

Walk through the four panels left-to-right, top-to-bottom:

1. **Top-left -- $t$ above all peaks.** The water covers the entire signal. No part of the curve pokes above the surface, so the upper-level set $U_t$ is empty. There are zero connected components and nothing has been born yet.

2. **Top-right -- $t$ just below peak A.** The water drops past the tallest peak (channel 60, amplitude $\approx 2.5$). A single island emerges: connected component A is **born** at $b_A = f(60)$. Because A is the global maximum it will never merge into anything else -- its persistence equals its full height. The annotation $\pi = 2.58$ reflects this: it is the most significant feature in the signal by a wide margin.

3. **Bottom-left -- $t$ just below peak B.** The water continues to descend and now drops past the second-tallest peak (channel 100, amplitude $\approx 1.4$). A second island appears: component B is **born** at $b_B = f(100)$. At this moment two disconnected islands coexist. Peak C (channel 140) is still submerged. Note that both born peaks already carry their final persistence annotations -- persistence is determined entirely by when a peak is born and when it eventually dies, not by the current water level.

4. **Bottom-right -- $t$ below the saddle between A and C.** The water has dropped far enough that the valley between peaks A and C fills in and their islands connect. Because C has a lower maximum than A, C is the **younger** component: it **dies** at the merge threshold. Its persistence $\pi_C = b_C - d_C = 0.73$ measures the height difference between its peak and the saddle where it was absorbed. Meanwhile peak B, sitting in a separate valley, remains an independent island with $\pi_B = 1.20$. The algorithm continues until only one component (the global maximum) survives.

The key insight is that **persistence directly encodes significance**: peak A ($\pi = 2.58$) is clearly the dominant feature, peak B ($\pi = 1.20$) is a solid secondary detection, and peak C ($\pi = 0.73$) is weaker but still well above the noise floor. Noise fluctuations, by contrast, produce tiny islands that merge almost immediately, resulting in persistence values close to zero. Setting a threshold $\pi_{\min} = \beta \times \sigma_{\rm rms}$ cleanly separates real peaks from noise without any smoothing or derivative computation.

### The persistence diagram

The persistence diagram provides a compact summary of every birth-death event in the filtration. Each peak is plotted as a point $(b, d)$ where $b$ is the birth value (peak height) and $d$ is the death value (merge threshold). The diagonal line $b = d$ represents zero persistence -- a peak born and immediately killed. The farther a point sits from the diagonal, the more significant the peak.

![Persistence diagram](/img/results/persistence-diagram.png)

*Generated by `uv run benchmarks persistence-plot`. See [Reproducing results](../results/reproducing).*

In this diagram, the three real peaks (red points) are clearly separated from the cluster of noise points hugging the diagonal. The dashed line parallel to the diagonal marks the persistence threshold $\pi_{\min} = \beta \times \sigma_{\rm rms}$. For this synthetic signal ($\sigma_{\rm rms} = 0.08$, $\beta = 3.8$), the threshold is $\pi_{\min} = 0.30$. Points below this line -- farther from the diagonal -- have persistence exceeding $\pi_{\min}$ and are retained as real peaks. Points above the line are rejected as noise.

The visual gap between signal and noise in the diagram illustrates why $\beta$ is insensitive: any threshold line drawn through this gap selects the same three peaks. Moving $\beta$ from 3.8 to 4.5 shifts the line slightly but does not change which points it separates -- the result is stable across a wide range of thresholds.

This is the fundamental advantage over derivative-based methods: instead of relying on regularization to suppress noise (where too little creates false peaks and too much destroys real ones), persistence ranks every candidate peak by an intrinsic topological measure that is robust to the noise level.

### From persistence to Gaussian candidates

The persistence filtration produces a list of `PersistentPeak` objects, each carrying five pieces of information: the **channel index** of the local maximum, its **birth** value (peak height), its **death** value (merge threshold), its **persistence** ($\pi = b - d$), and the **saddle index** (the channel where the component merged into an older neighbor). This is the raw output of the topology -- but these are not yet Gaussian fits. The next step is to convert them into initial guesses that a nonlinear least-squares solver can refine.

For each peak that survives the persistence threshold $\pi_{\min} = \beta \times \sigma_{\rm rms}$:

1. **Amplitude** is set to the signal value at the peak's channel index: $a_0 = f(\text{index})$.

2. **Mean** (center position) is set to the peak's channel index itself. The persistence algorithm identifies exactly which sample is the local maximum, so no interpolation is needed -- the index is already the best discrete estimate of the peak location.

3. **Standard deviation** is estimated from the peak-to-saddle distance. The saddle is the channel where the component died (merged into a taller neighbor), so $d = |\text{peak\_index} - \text{saddle\_index}|$ gives a one-sided extent. For a Gaussian with peak value $b$ (birth) that drops to $d_v$ (death) at distance $d$, the exact relationship is:

$$
\sigma_0 = \frac{d}{\sqrt{2 \ln(b / d_v)}}
$$

This provides a physically grounded initial width from the topology alone. For the global maximum (which never dies and has no saddle), the estimate falls back to $\sigma_0 = 1.0$.

The peaks are ordered by persistence (most significant first), so if a `max_components` cap is set, the least significant surviving peaks are dropped. This ordering also means the solver starts with the strongest features anchored in place, which improves convergence.

This initial guess is then passed to `scipy.optimize.curve_fit`, which fits a sum of Gaussians to the full signal:

$$
F(x, \mathbf{a}, \mathbf{\mu}, \mathbf{\sigma}) = \sum_i A_i \exp\left(-\frac{(x - \mu_i)^2}{2\sigma_i^2}\right).
$$

The solver adjusts all three parameters per component simultaneously (with bounds: $a \geq 0$, $\mu \in [0, n)$, $\sigma \in [0.3, n/2]$). Because the initial positions and amplitudes are already close to the truth -- persistence detected the right peaks -- the fit typically converges in few iterations, and the solver's main job is to determine the correct widths and fine-tune the positions and amplitudes. That being said, optimizing this function is the slowest step in the pipeline, which is why the refinement loop is designed to minimize unnecessary calls to `curve_fit` by only accepting changes that improve the AICc.

## The algorithm in PHSpectra

The implementation lives in `phspectra/src/persistence.py` and follows the union-find approach described above. Here is the full pipeline from raw spectrum to fitted Gaussians:

```mermaid
flowchart TD
    A["Raw 1-D spectrum"] --> B["Estimate sigma_rms"]
    B --> C["Persistence peak detection<br/>threshold = beta * sigma_rms"]
    C --> D["Convert peaks to<br/>initial Gaussian guesses<br/>(amplitude, mean, sigma from saddle)"]
    D --> E["Least-squares fit<br/>(scipy curve_fit)"]
    E --> F{"Refine?"}
    F -- No --> G["Return components"]
    F -- Yes --> H["Validate components<br/>(SNR, matched-filter SNR, FWHM)"]
    H --> I["Refit if any removed"]
    I --> J["Compute AICc baseline"]
    J --> K["Refinement loop<br/>(up to 3 iterations)"]
    K --> L["Search residual<br/>for missed peaks"]
    K --> M["Split broad component<br/>at negative dip"]
    K --> N["Merge blended pairs"]
    L & M & N --> R["Refit all components<br/>(scipy curve_fit)"]
    R --> Q{"AICc improved?"}
    Q -- Yes --> O["Accept change,<br/>continue loop"]
    Q -- No --> P["Reject change"]
    O --> K
    P --> G

    style E fill:#e74c3c,color:#fff,stroke:#c0392b
    style R fill:#e74c3c,color:#fff,stroke:#c0392b
```

Red nodes mark the computational bottlenecks. Both are `scipy.optimize.curve_fit` calls -- the initial fit runs once, but the refinement refit can be called up to three times per iteration (once per refinement operation: residual search, dip split, blended merge), each triggering a full nonlinear least-squares solve over all components.

Note that component validation happens **once** after the initial fit, not inside the refinement loop. If any components are rejected, a refit is triggered before computing the AICc baseline.

### Step 1: Noise estimation

Before detecting peaks, PHSpectra estimates $\sigma_{\rm rms}$ using a signal-masked approach ([Riener et al. 2019](https://arxiv.org/abs/1906.10506), Sect 3.1.1):

1. **Mask signal regions** -- runs of $> 2$ consecutive positive channels are masked (with 2-channel padding on each side) to exclude spectral features.
2. **MAD from negative channels** -- the median absolute deviation of the remaining negative channels gives an initial robust scale estimate.
3. **Clip outliers** -- channels exceeding $\pm 5\sigma_{\rm MAD}$ are masked.
4. **Final RMS** -- $\sigma_{\rm rms} = \sqrt{\mathrm{mean}(x^2)}$ over surviving channels.

This is more robust than a simple MAD of the full signal because it avoids biasing the noise estimate with actual spectral features.

### Step 2: Persistence peak detection

Peaks are detected using 0-dimensional persistent homology with a minimum persistence threshold:

$$
\pi_{\min} = \beta \times \sigma_{\rm rms}
$$

where $\beta = 3.8$ is the default (the main tuning parameter). The algorithm:

1. Sort all channel indices by decreasing signal value.
2. Process each index: mark it visited, check if its left/right neighbors are already visited.
3. If a neighbor belongs to an existing component, merge via union-find. The younger component (lower peak) dies at the current signal value; the merge channel is recorded as the **saddle index**.
4. Record any peak whose persistence $\pi = b - d > \pi_{\min}$, along with its saddle index.
5. The global maximum is always recorded (it never dies; saddle index is undefined).

Peaks are returned sorted by persistence, most significant first.

### Step 3: Initial Gaussian fit

Each detected peak becomes an initial guess for a Gaussian component:

| Parameter | Initial value |
| --------- | ------------- |
| Amplitude | Signal value at peak index |
| Mean | Peak channel index |
| Std. dev. | $d / \sqrt{2 \ln(b/d_v)}$ from peak-to-saddle distance (fallback: 1.0) |

These are fitted simultaneously as a sum of Gaussians via `scipy.optimize.curve_fit` with bounds (amplitude $\geq 0$, mean within spectrum, std. dev. $\in [0.3, n/2]$).

### Step 4: Validation and iterative refinement

When `refine=True` (default), the fitted components first pass through a **one-time validation** step. Components are rejected if they fail any of:

- FWHM $< 1$ channel
- Mean outside spectrum bounds
- Amplitude $< 1.5 \times \sigma_{\rm rms}$ (SNR floor)
- Matched-filter SNR $< 5.0$ (see below)

The **matched-filter SNR** is the optimal detection signal-to-noise ratio for a Gaussian component in white noise:

$$
\mathrm{SNR}_{\rm mf} = \frac{A_i}{\sigma_{\rm rms}} \sqrt{\sigma_i} \; \pi^{1/4}
$$

This follows from the matched-filter theorem: for a template $h(x) = A \exp(-x^2 / 2\sigma^2)$ in noise with standard deviation $\sigma_{\rm rms}$, the detection SNR is $\|h\|_2 / \sigma_{\rm rms}$, where $\|h\|_2^2 = A^2 \sigma \sqrt{\pi}$. Because SNR$_{\rm mf}$ scales as $\sqrt{\sigma}$, narrow peaks must have proportionally higher amplitude to survive -- this rejects noise spikes (which are tall but have negligible integrated flux) without imposing an ad-hoc minimum width. The amplitude SNR check (`snr_min`) and the matched-filter SNR check (`mf_snr_min`) are complementary: `snr_min` sets a floor for broad components, while `mf_snr_min` catches narrow noise.

If any components are removed, a refit is triggered. The AICc of the validated model then serves as the baseline for the refinement loop.

The **refinement loop** (up to 3 iterations) performs three operations per iteration, each accepted only if it lowers the corrected Akaike Information Criterion ([Akaike 1974](https://doi.org/10.1109/TAC.1974.1100705); small-sample correction by [Hurvich & Tsai 1989](https://doi.org/10.1093/biomet/76.2.297)):

$$
\mathrm{AICc} = N \ln\!\left(\frac{\mathrm{RSS}}{N}\right) + 2k + \frac{2k^2 + 2k}{N - k - 1}
$$

where $k = 3 \times$ (number of components) and $N$ is the number of channels.

**a) Residual peak search** -- run persistence detection on the residual (data minus model) to find missed components, using a lower threshold of $1.5 \times \sigma_{\rm rms}$.

**b) Negative dip splitting** -- if the residual has a dip below $-5 \times \sigma_{\rm rms}$, the broadest overlapping component is split into two narrower Gaussians.

**c) Blended pair merging** -- if two components are separated by less than $1.2 \times \min(\mathrm{FWHM}_i, \mathrm{FWHM}_j)$, they are merged into a single flux-weighted component.

### Why this works for spectra

Radio-astronomical spectra are superpositions of Gaussian emission lines sitting on a noisy baseline. Persistent homology is well-suited because:

1. **No smoothing needed** -- derivatives require choosing a kernel width; persistence works directly on the raw signal.
2. **Multi-scale by construction** -- broad and narrow features are detected simultaneously, ranked by the same persistence measure.
3. **Single parameter** -- $\beta$ controls the noise/signal boundary in physically meaningful units ($\sigma$).
4. **Robust to noise** -- the signal-masked RMS estimation and persistence ranking naturally separate noise fluctuations from real components without explicit SNR heuristics at the detection stage.
